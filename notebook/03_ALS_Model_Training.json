{
	"name": "03_ALS_Model_Training",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPool1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/9eb93167-f550-49e6-a443-cfd940efdb45/resourceGroups/demo/providers/Microsoft.Synapse/workspaces/synapseai/bigDataPools/SparkPool1",
				"name": "SparkPool1",
				"type": "Spark",
				"endpoint": "https://synapseai.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"Copyright (c) Microsoft Corporation. \n",
					"Licensed under the MIT license. \n",
					"## Model Training Script for Synapse-AI-Retail-Recommender  \n",
					"Model Author (Data Scientist): Xiaoyong Zhu  \n",
					"  \n",
					"This script is an adapted script of the full Model Training script that can be found in `4. ML Model Building`. This is a slimmed down version that only has the required operations for producing a model that the Model Deployment Process and the RecommendationRefresh notebook can consume."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"import sys\n",
					"print(sys.version)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"import matplotlib.pyplot as plt\n",
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"from pyspark.sql.functions import unix_timestamp\n",
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.functions import *\n",
					"\n",
					"from pyspark.ml import Pipeline\n",
					"from pyspark.ml import PipelineModel\n",
					"from pyspark.ml.feature import RFormula\n",
					"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
					"from pyspark.ml.classification import LogisticRegression\n",
					"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
					"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.read.table(\"retailaidb.cleaned_dataset\")\n",
					"spark.sparkContext.setCheckpointDir('checkpoint/')"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# Filter only for Electronics items\n",
					"\n",
					"df = df.withColumn('category_code_new', df['category_code'].substr(0, 11))\n",
					"df = df.filter(\"category_code_new = 'electronics'\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"top_category = df.groupBy('category_code_new').count().sort('count', ascending=False).limit(5) # only keep top 5 categories\n",
					"top_category = top_category.withColumnRenamed(\"category_code\",\"category_code_tmp\")"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"item_to_save = df.groupBy('product_id', \"category_code\").count().sort('count', ascending=False)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"item_to_save = item_to_save.join(top_category, top_category.category_code_tmp == item_to_save.category_code).limit(20)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import *\n",
					"\n",
					"raw_df = df\n",
					"\n",
					"product_count = df.groupBy('product_id').count()\n",
					"product_count = product_count.filter(\"count >= 30000\").orderBy('count', ascending=False) #only counts when the product has 20000 views\n",
					"\n",
					"raw_df = raw_df.withColumnRenamed(\"product_id\",\"product_id_tmp\")\n",
					"raw_df = raw_df.join(product_count, raw_df.product_id_tmp == product_count.product_id)\n",
					"\n",
					"user_count = df.groupBy('user_id').count()\n",
					"user_count = user_count.filter(\"count >= 200\").orderBy('count', ascending=False) #only counts when the user has 100 clicks\n",
					"\n",
					"raw_df = raw_df.withColumnRenamed(\"user_id\",\"user_id_tmp\")\n",
					"raw_df = raw_df.join(user_count, raw_df.user_id_tmp == user_count.user_id)\n",
					"\n",
					"df = raw_df\n",
					"\n",
					"df = df.where(df.event_type == \"view\")\n",
					"df = df.drop(\"event_time\",\"category_code\",\"user_session\",\"price\",\"brand\",\"category_id\")\n",
					"df = df.groupBy([df.product_id, df.user_id]).count()"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"# save table for further use\n",
					"df.write.saveAsTable(\"retailaidb.cleaned_dataset_electronics\", mode=\"overwrite\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"#import the required functions and libraries\n",
					"from pyspark.sql.functions import *"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import IntegerType\n",
					"df = df.withColumn(\"user_id\", df[\"user_id\"].cast(IntegerType()))\n",
					"df = df.withColumn(\"product_id\", df[\"product_id\"].cast(IntegerType()))"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"#split the data into training and test datatset\n",
					"train,test=df.randomSplit([0.75,0.25])"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"#import ALS recommender function from pyspark ml library\n",
					"from pyspark.ml.recommendation import ALS\n",
					"#Training the recommender model using train datatset\n",
					"rec=ALS(maxIter=40,regParam=0.20,implicitPrefs = True, userCol='user_id',itemCol='product_id',ratingCol='count',nonnegative=True,coldStartStrategy=\"drop\", rank=25)\n",
					"#fit the model on train set\n",
					"rec_model=rec.fit(train)\n",
					"#making predictions on test set \n",
					"predicted_ratings=rec_model.transform(test)\n",
					"#columns in predicted ratings dataframe\n",
					"predicted_ratings.printSchema()"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"predicted_ratings_witherr=predicted_ratings.withColumn('err',abs(predicted_ratings[\"prediction\"] - predicted_ratings[\"count\"]))"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"#importing Regression Evaluator to measure RMSE\n",
					"from pyspark.ml.evaluation import RegressionEvaluator"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"#create Regressor evaluator object for measuring accuracy\n",
					"evaluator=RegressionEvaluator(metricName='rmse',predictionCol='prediction',labelCol='count')"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"#apply the RE on predictions dataframe to calculate RMSE\n",
					"rmse=evaluator.evaluate(predicted_ratings)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"# Save the model\n",
					"rec_model.write().overwrite().save(\"retailai_recommendation_model\")"
				],
				"execution_count": 20
			}
		]
	}
}